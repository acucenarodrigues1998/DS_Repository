{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 08 - Multivariate Linear Regression\n",
    "\n",
    "* Hypotesis:\n",
    "    \n",
    "    \n",
    "    $y = b_0 + b_1 * x_1 + b_2 * x_2 + ...$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from Codes.ch01_load_and_convert_data import load_csv, str_column_to_float\n",
    "from Codes.ch02_scale_data_functions import dataset_minmax, normalize_dataset\n",
    "from Codes.ch03_resampling_methods import cross_validation_split\n",
    "from Codes.ch06_algorithm_test_harnesses import evaluate_algorithm_kfold_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with coefficients\n",
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefficients[i+1] * row[i]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Expected=1.000, Predicted=1.200\nExpected=3.000, Predicted=2.000\nExpected=3.000, Predicted=3.600\nExpected=2.000, Predicted=2.800\nExpected=5.000, Predicted=4.400\n"
     ]
    }
   ],
   "source": [
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "coef = [0.4, 0.8]\n",
    "for row in dataset:\n",
    "    yhat = predict(row, coef)\n",
    "    print(\"Expected=%.3f, Predicted=%.3f\" % (row[-1], yhat))"
   ]
  },
  {
   "source": [
    "### Estimating Coefficients\n",
    "\n",
    "Requires two parameters:\n",
    "\n",
    "* Learning Rate: Used to limit the amount that each coefficient is corrected each time it is updated. \n",
    "* Epochs: The number of times to run through the training data while updating the coefficients.\n",
    "\n",
    "Calculating these parameters:\n",
    "\n",
    "* $b_0(t + 1) = b_0(t) - Learning Rate * error(t)$\n",
    "* $b_n(t + 1) = b_n(t) - Learning Rate * error(t) * x_n(t)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = yhat - row[-1]\n",
    "            sum_error += error**2\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i+1] = coef[i+1] - l_rate * error * row[i]\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f'% (epoch, l_rate, sum_error))\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">epoch=0, lrate=0.001, error=46.236\n>epoch=1, lrate=0.001, error=41.305\n>epoch=2, lrate=0.001, error=36.930\n>epoch=3, lrate=0.001, error=33.047\n>epoch=4, lrate=0.001, error=29.601\n>epoch=5, lrate=0.001, error=26.543\n>epoch=6, lrate=0.001, error=23.830\n>epoch=7, lrate=0.001, error=21.422\n>epoch=8, lrate=0.001, error=19.285\n>epoch=9, lrate=0.001, error=17.389\n>epoch=10, lrate=0.001, error=15.706\n>epoch=11, lrate=0.001, error=14.213\n>epoch=12, lrate=0.001, error=12.888\n>epoch=13, lrate=0.001, error=11.712\n>epoch=14, lrate=0.001, error=10.668\n>epoch=15, lrate=0.001, error=9.742\n>epoch=16, lrate=0.001, error=8.921\n>epoch=17, lrate=0.001, error=8.191\n>epoch=18, lrate=0.001, error=7.544\n>epoch=19, lrate=0.001, error=6.970\n>epoch=20, lrate=0.001, error=6.461\n>epoch=21, lrate=0.001, error=6.009\n>epoch=22, lrate=0.001, error=5.607\n>epoch=23, lrate=0.001, error=5.251\n>epoch=24, lrate=0.001, error=4.935\n>epoch=25, lrate=0.001, error=4.655\n>epoch=26, lrate=0.001, error=4.406\n>epoch=27, lrate=0.001, error=4.186\n>epoch=28, lrate=0.001, error=3.990\n>epoch=29, lrate=0.001, error=3.816\n>epoch=30, lrate=0.001, error=3.662\n>epoch=31, lrate=0.001, error=3.525\n>epoch=32, lrate=0.001, error=3.404\n>epoch=33, lrate=0.001, error=3.296\n>epoch=34, lrate=0.001, error=3.200\n>epoch=35, lrate=0.001, error=3.115\n>epoch=36, lrate=0.001, error=3.040\n>epoch=37, lrate=0.001, error=2.973\n>epoch=38, lrate=0.001, error=2.914\n>epoch=39, lrate=0.001, error=2.862\n>epoch=40, lrate=0.001, error=2.815\n>epoch=41, lrate=0.001, error=2.773\n>epoch=42, lrate=0.001, error=2.737\n>epoch=43, lrate=0.001, error=2.704\n>epoch=44, lrate=0.001, error=2.675\n>epoch=45, lrate=0.001, error=2.650\n>epoch=46, lrate=0.001, error=2.627\n>epoch=47, lrate=0.001, error=2.607\n>epoch=48, lrate=0.001, error=2.589\n>epoch=49, lrate=0.001, error=2.573\n[0.22998234937311363, 0.8017220304137576]\n"
     ]
    }
   ],
   "source": [
    "# Calculate coefficients\n",
    "dataset = [[1, 1], [2, 3], [4, 3], [3, 2], [5, 5]]\n",
    "l_rate = 0.001\n",
    "n_epoch = 50\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ]
  },
  {
   "source": [
    "### Wine Quality Case Study"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Algorithm with Stochastic Gradient Descent\n",
    "def linear_regression_sgd(train, test, l_rate, n_epoch):\n",
    "    predictions = list()\n",
    "    coef = coefficients_sgd(train, l_rate, n_epoch)\n",
    "    for row in test:\n",
    "        yhat = predict(row, coef)\n",
    "        predictions.append(yhat)\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">epoch=0, lrate=0.010, error=79.062\n",
      ">epoch=1, lrate=0.010, error=67.512\n",
      ">epoch=2, lrate=0.010, error=65.919\n",
      ">epoch=3, lrate=0.010, error=65.037\n",
      ">epoch=4, lrate=0.010, error=64.511\n",
      ">epoch=5, lrate=0.010, error=64.185\n",
      ">epoch=6, lrate=0.010, error=63.975\n",
      ">epoch=7, lrate=0.010, error=63.836\n",
      ">epoch=8, lrate=0.010, error=63.742\n",
      ">epoch=9, lrate=0.010, error=63.676\n",
      ">epoch=10, lrate=0.010, error=63.628\n",
      ">epoch=11, lrate=0.010, error=63.593\n",
      ">epoch=12, lrate=0.010, error=63.566\n",
      ">epoch=13, lrate=0.010, error=63.544\n",
      ">epoch=14, lrate=0.010, error=63.527\n",
      ">epoch=15, lrate=0.010, error=63.512\n",
      ">epoch=16, lrate=0.010, error=63.500\n",
      ">epoch=17, lrate=0.010, error=63.489\n",
      ">epoch=18, lrate=0.010, error=63.479\n",
      ">epoch=19, lrate=0.010, error=63.469\n",
      ">epoch=20, lrate=0.010, error=63.461\n",
      ">epoch=21, lrate=0.010, error=63.452\n",
      ">epoch=22, lrate=0.010, error=63.444\n",
      ">epoch=23, lrate=0.010, error=63.437\n",
      ">epoch=24, lrate=0.010, error=63.430\n",
      ">epoch=25, lrate=0.010, error=63.423\n",
      ">epoch=26, lrate=0.010, error=63.416\n",
      ">epoch=27, lrate=0.010, error=63.409\n",
      ">epoch=28, lrate=0.010, error=63.402\n",
      ">epoch=29, lrate=0.010, error=63.396\n",
      ">epoch=30, lrate=0.010, error=63.390\n",
      ">epoch=31, lrate=0.010, error=63.383\n",
      ">epoch=32, lrate=0.010, error=63.377\n",
      ">epoch=33, lrate=0.010, error=63.371\n",
      ">epoch=34, lrate=0.010, error=63.365\n",
      ">epoch=35, lrate=0.010, error=63.359\n",
      ">epoch=36, lrate=0.010, error=63.353\n",
      ">epoch=37, lrate=0.010, error=63.348\n",
      ">epoch=38, lrate=0.010, error=63.342\n",
      ">epoch=39, lrate=0.010, error=63.336\n",
      ">epoch=40, lrate=0.010, error=63.331\n",
      ">epoch=41, lrate=0.010, error=63.325\n",
      ">epoch=42, lrate=0.010, error=63.320\n",
      ">epoch=43, lrate=0.010, error=63.314\n",
      ">epoch=44, lrate=0.010, error=63.309\n",
      ">epoch=45, lrate=0.010, error=63.304\n",
      ">epoch=46, lrate=0.010, error=63.298\n",
      ">epoch=47, lrate=0.010, error=63.293\n",
      ">epoch=48, lrate=0.010, error=63.288\n",
      ">epoch=49, lrate=0.010, error=63.283\n",
      ">epoch=0, lrate=0.010, error=78.440\n",
      ">epoch=1, lrate=0.010, error=65.663\n",
      ">epoch=2, lrate=0.010, error=64.090\n",
      ">epoch=3, lrate=0.010, error=63.227\n",
      ">epoch=4, lrate=0.010, error=62.717\n",
      ">epoch=5, lrate=0.010, error=62.404\n",
      ">epoch=6, lrate=0.010, error=62.205\n",
      ">epoch=7, lrate=0.010, error=62.074\n",
      ">epoch=8, lrate=0.010, error=61.986\n",
      ">epoch=9, lrate=0.010, error=61.924\n",
      ">epoch=10, lrate=0.010, error=61.880\n",
      ">epoch=11, lrate=0.010, error=61.848\n",
      ">epoch=12, lrate=0.010, error=61.823\n",
      ">epoch=13, lrate=0.010, error=61.804\n",
      ">epoch=14, lrate=0.010, error=61.788\n",
      ">epoch=15, lrate=0.010, error=61.775\n",
      ">epoch=16, lrate=0.010, error=61.763\n",
      ">epoch=17, lrate=0.010, error=61.753\n",
      ">epoch=18, lrate=0.010, error=61.744\n",
      ">epoch=19, lrate=0.010, error=61.735\n",
      ">epoch=20, lrate=0.010, error=61.728\n",
      ">epoch=21, lrate=0.010, error=61.720\n",
      ">epoch=22, lrate=0.010, error=61.713\n",
      ">epoch=23, lrate=0.010, error=61.706\n",
      ">epoch=24, lrate=0.010, error=61.699\n",
      ">epoch=25, lrate=0.010, error=61.692\n",
      ">epoch=26, lrate=0.010, error=61.686\n",
      ">epoch=27, lrate=0.010, error=61.679\n",
      ">epoch=28, lrate=0.010, error=61.673\n",
      ">epoch=29, lrate=0.010, error=61.667\n",
      ">epoch=30, lrate=0.010, error=61.661\n",
      ">epoch=31, lrate=0.010, error=61.655\n",
      ">epoch=32, lrate=0.010, error=61.649\n",
      ">epoch=33, lrate=0.010, error=61.643\n",
      ">epoch=34, lrate=0.010, error=61.637\n",
      ">epoch=35, lrate=0.010, error=61.631\n",
      ">epoch=36, lrate=0.010, error=61.626\n",
      ">epoch=37, lrate=0.010, error=61.620\n",
      ">epoch=38, lrate=0.010, error=61.614\n",
      ">epoch=39, lrate=0.010, error=61.609\n",
      ">epoch=40, lrate=0.010, error=61.603\n",
      ">epoch=41, lrate=0.010, error=61.597\n",
      ">epoch=42, lrate=0.010, error=61.592\n",
      ">epoch=43, lrate=0.010, error=61.586\n",
      ">epoch=44, lrate=0.010, error=61.581\n",
      ">epoch=45, lrate=0.010, error=61.576\n",
      ">epoch=46, lrate=0.010, error=61.570\n",
      ">epoch=47, lrate=0.010, error=61.565\n",
      ">epoch=48, lrate=0.010, error=61.560\n",
      ">epoch=49, lrate=0.010, error=61.554\n",
      ">epoch=0, lrate=0.010, error=79.834\n",
      ">epoch=1, lrate=0.010, error=66.690\n",
      ">epoch=2, lrate=0.010, error=64.987\n",
      ">epoch=3, lrate=0.010, error=64.069\n",
      ">epoch=4, lrate=0.010, error=63.535\n",
      ">epoch=5, lrate=0.010, error=63.211\n",
      ">epoch=6, lrate=0.010, error=63.008\n",
      ">epoch=7, lrate=0.010, error=62.877\n",
      ">epoch=8, lrate=0.010, error=62.790\n",
      ">epoch=9, lrate=0.010, error=62.731\n",
      ">epoch=10, lrate=0.010, error=62.690\n",
      ">epoch=11, lrate=0.010, error=62.660\n",
      ">epoch=12, lrate=0.010, error=62.638\n",
      ">epoch=13, lrate=0.010, error=62.622\n",
      ">epoch=14, lrate=0.010, error=62.609\n",
      ">epoch=15, lrate=0.010, error=62.598\n",
      ">epoch=16, lrate=0.010, error=62.590\n",
      ">epoch=17, lrate=0.010, error=62.582\n",
      ">epoch=18, lrate=0.010, error=62.575\n",
      ">epoch=19, lrate=0.010, error=62.569\n",
      ">epoch=20, lrate=0.010, error=62.563\n",
      ">epoch=21, lrate=0.010, error=62.558\n",
      ">epoch=22, lrate=0.010, error=62.552\n",
      ">epoch=23, lrate=0.010, error=62.547\n",
      ">epoch=24, lrate=0.010, error=62.543\n",
      ">epoch=25, lrate=0.010, error=62.538\n",
      ">epoch=26, lrate=0.010, error=62.533\n",
      ">epoch=27, lrate=0.010, error=62.529\n",
      ">epoch=28, lrate=0.010, error=62.524\n",
      ">epoch=29, lrate=0.010, error=62.520\n",
      ">epoch=30, lrate=0.010, error=62.516\n",
      ">epoch=31, lrate=0.010, error=62.511\n",
      ">epoch=32, lrate=0.010, error=62.507\n",
      ">epoch=33, lrate=0.010, error=62.503\n",
      ">epoch=34, lrate=0.010, error=62.499\n",
      ">epoch=35, lrate=0.010, error=62.495\n",
      ">epoch=36, lrate=0.010, error=62.491\n",
      ">epoch=37, lrate=0.010, error=62.487\n",
      ">epoch=38, lrate=0.010, error=62.483\n",
      ">epoch=39, lrate=0.010, error=62.479\n",
      ">epoch=40, lrate=0.010, error=62.475\n",
      ">epoch=41, lrate=0.010, error=62.471\n",
      ">epoch=42, lrate=0.010, error=62.467\n",
      ">epoch=43, lrate=0.010, error=62.463\n",
      ">epoch=44, lrate=0.010, error=62.460\n",
      ">epoch=45, lrate=0.010, error=62.456\n",
      ">epoch=46, lrate=0.010, error=62.452\n",
      ">epoch=47, lrate=0.010, error=62.448\n",
      ">epoch=48, lrate=0.010, error=62.445\n",
      ">epoch=49, lrate=0.010, error=62.441\n",
      ">epoch=0, lrate=0.010, error=79.207\n",
      ">epoch=1, lrate=0.010, error=66.076\n",
      ">epoch=2, lrate=0.010, error=64.391\n",
      ">epoch=3, lrate=0.010, error=63.483\n",
      ">epoch=4, lrate=0.010, error=62.954\n",
      ">epoch=5, lrate=0.010, error=62.634\n",
      ">epoch=6, lrate=0.010, error=62.434\n",
      ">epoch=7, lrate=0.010, error=62.304\n",
      ">epoch=8, lrate=0.010, error=62.217\n",
      ">epoch=9, lrate=0.010, error=62.157\n",
      ">epoch=10, lrate=0.010, error=62.115\n",
      ">epoch=11, lrate=0.010, error=62.084\n",
      ">epoch=12, lrate=0.010, error=62.061\n",
      ">epoch=13, lrate=0.010, error=62.044\n",
      ">epoch=14, lrate=0.010, error=62.030\n",
      ">epoch=15, lrate=0.010, error=62.018\n",
      ">epoch=16, lrate=0.010, error=62.008\n",
      ">epoch=17, lrate=0.010, error=62.000\n",
      ">epoch=18, lrate=0.010, error=61.992\n",
      ">epoch=19, lrate=0.010, error=61.985\n",
      ">epoch=20, lrate=0.010, error=61.978\n",
      ">epoch=21, lrate=0.010, error=61.972\n",
      ">epoch=22, lrate=0.010, error=61.966\n",
      ">epoch=23, lrate=0.010, error=61.961\n",
      ">epoch=24, lrate=0.010, error=61.956\n",
      ">epoch=25, lrate=0.010, error=61.950\n",
      ">epoch=26, lrate=0.010, error=61.945\n",
      ">epoch=27, lrate=0.010, error=61.940\n",
      ">epoch=28, lrate=0.010, error=61.935\n",
      ">epoch=29, lrate=0.010, error=61.931\n",
      ">epoch=30, lrate=0.010, error=61.926\n",
      ">epoch=31, lrate=0.010, error=61.921\n",
      ">epoch=32, lrate=0.010, error=61.917\n",
      ">epoch=33, lrate=0.010, error=61.912\n",
      ">epoch=34, lrate=0.010, error=61.908\n",
      ">epoch=35, lrate=0.010, error=61.903\n",
      ">epoch=36, lrate=0.010, error=61.899\n",
      ">epoch=37, lrate=0.010, error=61.895\n",
      ">epoch=38, lrate=0.010, error=61.890\n",
      ">epoch=39, lrate=0.010, error=61.886\n",
      ">epoch=40, lrate=0.010, error=61.882\n",
      ">epoch=41, lrate=0.010, error=61.878\n",
      ">epoch=42, lrate=0.010, error=61.874\n",
      ">epoch=43, lrate=0.010, error=61.870\n",
      ">epoch=44, lrate=0.010, error=61.866\n",
      ">epoch=45, lrate=0.010, error=61.862\n",
      ">epoch=46, lrate=0.010, error=61.858\n",
      ">epoch=47, lrate=0.010, error=61.854\n",
      ">epoch=48, lrate=0.010, error=61.850\n",
      ">epoch=49, lrate=0.010, error=61.846\n",
      ">epoch=0, lrate=0.010, error=80.522\n",
      ">epoch=1, lrate=0.010, error=67.386\n",
      ">epoch=2, lrate=0.010, error=65.629\n",
      ">epoch=3, lrate=0.010, error=64.670\n",
      ">epoch=4, lrate=0.010, error=64.107\n",
      ">epoch=5, lrate=0.010, error=63.765\n",
      ">epoch=6, lrate=0.010, error=63.549\n",
      ">epoch=7, lrate=0.010, error=63.409\n",
      ">epoch=8, lrate=0.010, error=63.315\n",
      ">epoch=9, lrate=0.010, error=63.250\n",
      ">epoch=10, lrate=0.010, error=63.204\n",
      ">epoch=11, lrate=0.010, error=63.170\n",
      ">epoch=12, lrate=0.010, error=63.144\n",
      ">epoch=13, lrate=0.010, error=63.124\n",
      ">epoch=14, lrate=0.010, error=63.107\n",
      ">epoch=15, lrate=0.010, error=63.094\n",
      ">epoch=16, lrate=0.010, error=63.082\n",
      ">epoch=17, lrate=0.010, error=63.072\n",
      ">epoch=18, lrate=0.010, error=63.063\n",
      ">epoch=19, lrate=0.010, error=63.054\n",
      ">epoch=20, lrate=0.010, error=63.046\n",
      ">epoch=21, lrate=0.010, error=63.039\n",
      ">epoch=22, lrate=0.010, error=63.032\n",
      ">epoch=23, lrate=0.010, error=63.026\n",
      ">epoch=24, lrate=0.010, error=63.020\n",
      ">epoch=25, lrate=0.010, error=63.014\n",
      ">epoch=26, lrate=0.010, error=63.008\n",
      ">epoch=27, lrate=0.010, error=63.002\n",
      ">epoch=28, lrate=0.010, error=62.997\n",
      ">epoch=29, lrate=0.010, error=62.991\n",
      ">epoch=30, lrate=0.010, error=62.986\n",
      ">epoch=31, lrate=0.010, error=62.981\n",
      ">epoch=32, lrate=0.010, error=62.976\n",
      ">epoch=33, lrate=0.010, error=62.971\n",
      ">epoch=34, lrate=0.010, error=62.966\n",
      ">epoch=35, lrate=0.010, error=62.961\n",
      ">epoch=36, lrate=0.010, error=62.957\n",
      ">epoch=37, lrate=0.010, error=62.952\n",
      ">epoch=38, lrate=0.010, error=62.948\n",
      ">epoch=39, lrate=0.010, error=62.943\n",
      ">epoch=40, lrate=0.010, error=62.939\n",
      ">epoch=41, lrate=0.010, error=62.934\n",
      ">epoch=42, lrate=0.010, error=62.930\n",
      ">epoch=43, lrate=0.010, error=62.926\n",
      ">epoch=44, lrate=0.010, error=62.921\n",
      ">epoch=45, lrate=0.010, error=62.917\n",
      ">epoch=46, lrate=0.010, error=62.913\n",
      ">epoch=47, lrate=0.010, error=62.909\n",
      ">epoch=48, lrate=0.010, error=62.905\n",
      ">epoch=49, lrate=0.010, error=62.901\n",
      "Scores: [0.12248058224159092, 0.13034017509167112, 0.12620370547483578, 0.12897687952843237, 0.12446990678682233]\n",
      "Mean RMSE: 0.126\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression on wine quality dataset\n",
    "seed(1)\n",
    "\n",
    "# Load and prepare data\n",
    "filename = './data/winequality-white.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i)\n",
    "\n",
    "# Normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "\n",
    "# Evaluate algorithm\n",
    "n_folds = 5\n",
    "l_rate = 0.01\n",
    "n_epoch = 50\n",
    "scores = evaluate_algorithm_kfold_reg(dataset, linear_regression_sgd, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean RMSE: %.3f' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "source": [
    "## Future Works\n",
    "\n",
    "* Tune The Example. Tune the learning rate, number of epochs and even the data\n",
    "preparation method to get an improved score on the Wine Quality dataset.\n",
    "* Batch Stochastic Gradient Descent. Change the stochastic gradient descent algorithm\n",
    "to accumulate updates across each epoch and only update the coefficients in a batch at\n",
    "the end of the epoch.\n",
    "* Additional Regression Problems. Apply the technique to other regression problems\n",
    "on the UCI machine learning repository."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
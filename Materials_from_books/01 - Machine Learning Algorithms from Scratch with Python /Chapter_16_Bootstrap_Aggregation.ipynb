{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 16 - Bootstrap Aggregation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed, randrange"
   ]
  },
  {
   "source": [
    "### Bootstrap Resample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio=1.0):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset)*ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True Mean: 4.500\nSamples=1, Estimated Mean: 4.000\nSamples=10, Estimated Mean: 4.700\nSamples=100, Estimated Mean: 4.570\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "    return sum(numbers) / float(len(numbers))\n",
    "\n",
    "# Test subsampling a dataset\n",
    "seed(1)\n",
    "# True mean\n",
    "dataset = [[randrange(10)] for i in range(20)]\n",
    "print('True Mean: %.3f' % mean([row[0] for row in dataset]))\n",
    "# Estimated means\n",
    "ratio = 0.10\n",
    "for size in [1, 10, 100]:\n",
    "    sample_means = list()\n",
    "    for i in range(size):\n",
    "        sample = subsample(dataset, ratio)\n",
    "        sample_mean = mean([row[0] for row in sample])\n",
    "        sample_means.append(sample_mean)\n",
    "    print('Samples=%d, Estimated Mean: %.3f' % (size, mean(sample_means)))"
   ]
  },
  {
   "source": [
    "### Sonar Case Study"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load and prepare data\n",
    "from Codes.ch01_load_and_convert_data import load_csv, str_column_to_float, str_column_to_int\n",
    "# k-Fold for classification evaluation\n",
    "from Codes.ch06_algorithm_test_harnesses import evaluate_algorithm_kfold\n",
    "# Decision Tree Classifier\n",
    "from Codes.ch11_decision_tree import test_split, gini_index, get_split, to_terminal, split, build_tree, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Bootstrap Aggregation Algorithm\n",
    "def bagging(train, test, max_depth, min_size, sample_size, n_trees):\n",
    "    trees = list()\n",
    "    for _ in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size)\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trees: 1\n",
      "Scores: [87.8048780487805, 68.29268292682927, 65.85365853658537, 65.85365853658537, 73.17073170731707]\n",
      "Mean Accuracy: 72.195%\n",
      "Trees: 5\n",
      "Scores: [63.41463414634146, 80.48780487804879, 78.04878048780488, 82.92682926829268, 60.97560975609756]\n",
      "Mean Accuracy: 73.171%\n",
      "Trees: 10\n",
      "Scores: [58.536585365853654, 78.04878048780488, 80.48780487804879, 85.36585365853658, 65.85365853658537]\n",
      "Mean Accuracy: 73.659%\n",
      "Trees: 50\n",
      "Scores: [60.97560975609756, 75.60975609756098, 82.92682926829268, 73.17073170731707, 85.36585365853658]\n",
      "Mean Accuracy: 75.610%\n"
     ]
    }
   ],
   "source": [
    "# Test bagging on the sonar dataset\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "filename = './data/sonar.all-data.csv'\n",
    "dataset = load_csv(filename)\n",
    "# convert string attributes to integers\n",
    "for i in range(len(dataset[0])-1):\n",
    "    str_column_to_float(dataset, i)\n",
    "# convert class column to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds = 5\n",
    "max_depth = 6\n",
    "min_size = 2\n",
    "sample_size = 0.50\n",
    "for n_trees in [1, 5, 10, 50]:\n",
    "    scores = evaluate_algorithm_kfold(dataset, bagging, n_folds, max_depth, min_size, sample_size, n_trees)\n",
    "    print('Trees: %d' % n_trees)\n",
    "    print('Scores: %s' % scores)\n",
    "    print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "source": [
    "## Future Works\n",
    "\n",
    "* Tune the Example. Explore different configurations for the number of trees and even individual tree configurations to see if you can further improve results.\n",
    "* Bag Another Algorithm. Other algorithms can be used with bagging. For example, a k-nearest neighbor algorithm with a low value of k will have a high variance and is a good candidate for bagging.\n",
    "* Regression Problems. Bagging can be used with regression trees. Instead of predicting the most common class value from the set of predictions, you can return the average of the predictions from the bagged trees. Experiment on regression problems."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
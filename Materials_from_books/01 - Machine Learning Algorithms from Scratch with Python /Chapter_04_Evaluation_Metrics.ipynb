{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 04 - Evaluation Metrics\n",
    "\n",
    "### Classification accuracy\n",
    "\n",
    "$accuracy = \\frac{correct predictions}{total predictions} * 100$\n",
    "\n",
    "0% is the worst possible accuracy  \n",
    "100% is the best possible accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "actual = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "predicted = [0, 1, 0, 0, 0, 1, 0, 1, 1, 1]\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "source": [
    "### Confusion Matrix\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a confusion matrix\n",
    "def confusion_matrix(actual, predicted):\n",
    "    unique = set(actual)\n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for x in range(len(unique))]\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i]]\n",
    "        y = lookup[predicted[i]]\n",
    "        matrix[y][x] += 1\n",
    "    return unique, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0, 1}\n[[3, 1], [2, 4]]\n"
     ]
    }
   ],
   "source": [
    "# Test confusion matrix with integers\n",
    "actual = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 0, 0, 1, 0, 1, 1, 1]\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "print(unique)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print a confusion matrix\n",
    "def print_confusion_matrix(unique, matrix):\n",
    "    print('(A)' + ' '.join(str(x) for x in unique))\n",
    "    print('(P)---')\n",
    "    for i, x in enumerate(unique):\n",
    "        print(\"%s| %s\" % (x, ' '.join(str(x) for x in matrix[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(A)0 1\n(P)---\n0| 3 1\n1| 2 4\n"
     ]
    }
   ],
   "source": [
    "# Test confusion matrix with integers\n",
    "actual = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "predicted = [0, 1, 1, 0, 0, 1, 0, 1, 1, 1]\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "print_confusion_matrix(unique, matrix)"
   ]
  },
  {
   "source": [
    "### Mean Absolute Error"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute error\n",
    "def mae_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_error += abs(predicted[i] - actual[i])\n",
    "    return sum_error / float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.007999999999999993\n"
     ]
    }
   ],
   "source": [
    "# Test MAE\n",
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
    "mae = mae_metric(actual, predicted)\n",
    "print(mae)"
   ]
  },
  {
   "source": [
    "### Root Mean Squared Error"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.00894427190999915\n"
     ]
    }
   ],
   "source": [
    "# Test RMSE\n",
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
    "rmse = rmse_metric(actual, predicted)\n",
    "print(rmse)"
   ]
  },
  {
   "source": [
    "## Future Works\n",
    "\n",
    "Implement these metrics:\n",
    "\n",
    "* Precision for classification.\n",
    "* Recall for classification.\n",
    "* F1 for classification.\n",
    "* Area Under ROC Curve or AUC for classification.\n",
    "* Goodness of Fit or R^2 (R squared) for regression."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}